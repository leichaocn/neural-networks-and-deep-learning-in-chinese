{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax与对数似然函数及它俩在PyTorch中的实现细节\n",
    "__filename__ = \"softmax_and_logliklihood_with_pytorch.ipynb\"\n",
    "\n",
    "__date__ = 2019 / 4 / 10\n",
    "\n",
    "__author__ = \"leichao\"\n",
    "\n",
    "__email__ = \"leichaocn@163.com\"\n",
    "\n",
    "---\n",
    "对于一个尺寸为(N,C)的模型输出数据，每一行表示一个样本，共N个样本；每一个样本有C列，表示一个样本分别对应的C个类的预测数值。\n",
    "对每一行样本，它有C个$z_i$值，我们对于每个$z_i$求取它的softmax值。\n",
    "经典的softmax为（对于一条样本的输出各维度）\n",
    "$$ a_{i}=\\frac{e^{z_{i}}}{\\sum{e^{z_{i}}}}\n",
    "$$\n",
    "经典的对数似然函数为（对于N条样本，每条样本仅拿出$y_i=1$对应的那个$a_i$）\n",
    "$$ loss=-\\sum{y_ilog(a_{i})}\n",
    "$$\n",
    "其中$y_i$是我们的预测值。$loss$是N条样本上的总损失。\n",
    "\n",
    "---\n",
    "在pytorch中有两种实现方式：\n",
    "\n",
    "**第一种：log_softmax()与nll_loss()组合**\n",
    "\n",
    "F.log_softmax()干的事是\n",
    "$$\n",
    "lsm_i=log(a_i)=log(\\frac{e^{z_{i}}}{\\sum{e^{z_{i}}}})=z_i-log(\\sum{e^{z_{i}}})\n",
    "$$\n",
    "F.nll_loss()干的事是\n",
    "$$ loss=-\\sum{y_ilsm_i}\n",
    "$$\n",
    "\n",
    "输入到log_softmax中的tensor的尺寸必须是NxC，\n",
    "每一行表示一个样本，共N个样本；每一个样本有C列，表示一个样本分别对应的C个类的预测数值。\n",
    "对每一行样本，它有C个$z_i$值，我们对于每个$z_i$求取它的softmax值。\n",
    "\n",
    "在代码`F.nll_loss(F.log_softmax(input),target)`中\n",
    "    - input的尺寸为（N，C），\n",
    "    - input的每一行表示一个样本，共N个样本；每一个样本有C列，表示一个样本分别对应的C个类的预测数值。\n",
    "    - input的每一行样本，它有C个$z_i$ 值，我们对于每个$z_i$求取它的softmax值。\n",
    "    - F.log_softmax(input)的输出尺寸为依然为（N，C）\n",
    "    - target的尺寸为N的一维张量，元素最小为0，最大为C-1\n",
    "    - nll_loss()的参数，如果reduction='sum'，则loss为全部N个样本的总损失；如果缺省，则loss为全部N个样本的总损失除以N。\n",
    "\n",
    "\n",
    "**第2种：cross_entropy()**\n",
    "\n",
    "`F.cross_entropy(input,target)`相当于把softmax与对数似然函数一起做了，它的功能等于`F.nll_loss(F.log_softmax(input),target)`\n",
    "\n",
    "对比代码见下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:53:19.018000Z",
     "start_time": "2019-09-17T15:53:05.930000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.size()= torch.Size([3, 2])\n",
      "input.item()= tensor([[-0.3641, -0.6573],\n",
      "        [ 2.0380,  0.0544],\n",
      "        [-0.5526,  1.8896]], requires_grad=True)\n",
      "target.size()= torch.Size([3])\n",
      "target.item()= tensor([0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_input.size()= torch.Size([3, 2])\n",
      "softmax_input.item()= tensor([[-0.5573, -0.8504],\n",
      "        [-0.1289, -2.1125],\n",
      "        [-2.5257, -0.0834]], grad_fn=<LogSoftmaxBackward>)\n",
      "nll_loss_out.size()= torch.Size([])\n",
      "nll_loss_out= 1.0706039667129517\n",
      "crossentropy_loss_out.size()= torch.Size([])\n",
      "crossentropy_loss_out= 1.0706039667129517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# input假设为模型softmax之前的输出结果，尺寸为（N，C），即N个样本分别对应C个类的预测结果。\n",
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "print('input.size()=',input.size())\n",
    "print('input.item()=',input)\n",
    "\n",
    "\n",
    "# target中有N个元素，元素的值的范围为[0,C-1]\n",
    "target = torch.tensor([0, 0, 0])\n",
    "print('target.size()=',target.size())\n",
    "print('target.item()=',target)\n",
    "\n",
    "\n",
    "softmax_input=F.log_softmax(input)\n",
    "print('softmax_input.size()=',softmax_input.size())\n",
    "print('softmax_input.item()=',softmax_input)\n",
    "\n",
    "nll_loss_out = F.nll_loss(softmax_input,target)\n",
    "# nll_loss_out = F.nll_loss(softmax_input,target,reduction='sum')\n",
    "\n",
    "crossentropy_loss_out = F.cross_entropy(input,target)\n",
    "# crossentropy_loss_out = F.cross_entropy(input,target,reduction='sum')\n",
    "\n",
    "print('nll_loss_out.size()=',nll_loss_out.size())\n",
    "print('nll_loss_out=',nll_loss_out.item())\n",
    "\n",
    "print('crossentropy_loss_out.size()=',crossentropy_loss_out.size())\n",
    "print('crossentropy_loss_out=',crossentropy_loss_out.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
